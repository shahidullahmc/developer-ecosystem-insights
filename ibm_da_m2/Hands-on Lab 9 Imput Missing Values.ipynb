{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Impute Missing Values**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will practice essential data wrangling techniques using the Stack Overflow survey dataset. The primary focus is on handling missing data and ensuring data quality. You will:\n",
    "\n",
    "- **Load the Data:** Import the dataset into a DataFrame using the pandas library.\n",
    "\n",
    "- **Clean the Data:** Identify and remove duplicate entries to maintain data integrity.\n",
    "\n",
    "- **Handle Missing Values:** Detect missing values, impute them with appropriate strategies, and verify the imputation to create a complete and reliable dataset for analysis.\n",
    "\n",
    "This lab equips you with the skills to effectively preprocess and clean real-world datasets, a crucial step in any data analysis project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will perform the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Identify missing values in the dataset.\n",
    "\n",
    "-   Apply techniques to impute missing values in the dataset.\n",
    "  \n",
    "-   Use suitable techniques to normalize data in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install needed library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-3.0.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-3.0.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (10.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m188.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m165.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: numpy, pandas\n",
      "Successfully installed numpy-2.4.2 pandas-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset Into a Dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Read Data**\n",
    "<p>\n",
    "The functions below will download the dataset into your browser:\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ResponseId                      MainBranch                 Age  \\\n",
      "0           1  I am a developer by profession  Under 18 years old   \n",
      "1           2  I am a developer by profession     35-44 years old   \n",
      "2           3  I am a developer by profession     45-54 years old   \n",
      "3           4           I am learning to code     18-24 years old   \n",
      "4           5  I am a developer by profession     18-24 years old   \n",
      "\n",
      "            Employment RemoteWork   Check  \\\n",
      "0  Employed, full-time     Remote  Apples   \n",
      "1  Employed, full-time     Remote  Apples   \n",
      "2  Employed, full-time     Remote  Apples   \n",
      "3   Student, full-time        NaN  Apples   \n",
      "4   Student, full-time        NaN  Apples   \n",
      "\n",
      "                                    CodingActivities  \\\n",
      "0                                              Hobby   \n",
      "1  Hobby;Contribute to open-source projects;Other...   \n",
      "2  Hobby;Contribute to open-source projects;Other...   \n",
      "3                                                NaN   \n",
      "4                                                NaN   \n",
      "\n",
      "                                             EdLevel  \\\n",
      "0                          Primary/elementary school   \n",
      "1       Bachelor’s degree (B.A., B.S., B.Eng., etc.)   \n",
      "2    Master’s degree (M.A., M.S., M.Eng., MBA, etc.)   \n",
      "3  Some college/university study without earning ...   \n",
      "4  Secondary school (e.g. American high school, G...   \n",
      "\n",
      "                                           LearnCode  \\\n",
      "0                             Books / Physical media   \n",
      "1  Books / Physical media;Colleague;On the job tr...   \n",
      "2  Books / Physical media;Colleague;On the job tr...   \n",
      "3  Other online resources (e.g., videos, blogs, f...   \n",
      "4  Other online resources (e.g., videos, blogs, f...   \n",
      "\n",
      "                                     LearnCodeOnline  ... JobSatPoints_6  \\\n",
      "0                                                NaN  ...            NaN   \n",
      "1  Technical documentation;Blogs;Books;Written Tu...  ...            0.0   \n",
      "2  Technical documentation;Blogs;Books;Written Tu...  ...            NaN   \n",
      "3  Stack Overflow;How-to videos;Interactive tutorial  ...            NaN   \n",
      "4  Technical documentation;Blogs;Written Tutorial...  ...            NaN   \n",
      "\n",
      "  JobSatPoints_7 JobSatPoints_8 JobSatPoints_9 JobSatPoints_10  \\\n",
      "0            NaN            NaN            NaN             NaN   \n",
      "1            0.0            0.0            0.0             0.0   \n",
      "2            NaN            NaN            NaN             NaN   \n",
      "3            NaN            NaN            NaN             NaN   \n",
      "4            NaN            NaN            NaN             NaN   \n",
      "\n",
      "  JobSatPoints_11           SurveyLength SurveyEase ConvertedCompYearly JobSat  \n",
      "0             NaN                    NaN        NaN                 NaN    NaN  \n",
      "1             0.0                    NaN        NaN                 NaN    NaN  \n",
      "2             NaN  Appropriate in length       Easy                 NaN    NaN  \n",
      "3             NaN               Too long       Easy                 NaN    NaN  \n",
      "4             NaN              Too short       Easy                 NaN    NaN  \n",
      "\n",
      "[5 rows x 114 columns]\n"
     ]
    }
   ],
   "source": [
    "file_path = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/n01PQ9pSmiRX6520flujwQ/survey-data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to ensure it loaded correctly\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Finding and Removing Duplicates\n",
    "##### Task 1: Identify duplicate rows in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ResponseId</th>\n",
       "      <th>MainBranch</th>\n",
       "      <th>Age</th>\n",
       "      <th>Employment</th>\n",
       "      <th>RemoteWork</th>\n",
       "      <th>Check</th>\n",
       "      <th>CodingActivities</th>\n",
       "      <th>EdLevel</th>\n",
       "      <th>LearnCode</th>\n",
       "      <th>LearnCodeOnline</th>\n",
       "      <th>...</th>\n",
       "      <th>JobSatPoints_6</th>\n",
       "      <th>JobSatPoints_7</th>\n",
       "      <th>JobSatPoints_8</th>\n",
       "      <th>JobSatPoints_9</th>\n",
       "      <th>JobSatPoints_10</th>\n",
       "      <th>JobSatPoints_11</th>\n",
       "      <th>SurveyLength</th>\n",
       "      <th>SurveyEase</th>\n",
       "      <th>ConvertedCompYearly</th>\n",
       "      <th>JobSat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ResponseId, MainBranch, Age, Employment, RemoteWork, Check, CodingActivities, EdLevel, LearnCode, LearnCodeOnline, TechDoc, YearsCode, YearsCodePro, DevType, OrgSize, PurchaseInfluence, BuyNewTool, BuildvsBuy, TechEndorse, Country, Currency, CompTotal, LanguageHaveWorkedWith, LanguageWantToWorkWith, LanguageAdmired, DatabaseHaveWorkedWith, DatabaseWantToWorkWith, DatabaseAdmired, PlatformHaveWorkedWith, PlatformWantToWorkWith, PlatformAdmired, WebframeHaveWorkedWith, WebframeWantToWorkWith, WebframeAdmired, EmbeddedHaveWorkedWith, EmbeddedWantToWorkWith, EmbeddedAdmired, MiscTechHaveWorkedWith, MiscTechWantToWorkWith, MiscTechAdmired, ToolsTechHaveWorkedWith, ToolsTechWantToWorkWith, ToolsTechAdmired, NEWCollabToolsHaveWorkedWith, NEWCollabToolsWantToWorkWith, NEWCollabToolsAdmired, OpSysPersonal use, OpSysProfessional use, OfficeStackAsyncHaveWorkedWith, OfficeStackAsyncWantToWorkWith, OfficeStackAsyncAdmired, OfficeStackSyncHaveWorkedWith, OfficeStackSyncWantToWorkWith, OfficeStackSyncAdmired, AISearchDevHaveWorkedWith, AISearchDevWantToWorkWith, AISearchDevAdmired, NEWSOSites, SOVisitFreq, SOAccount, SOPartFreq, SOHow, SOComm, AISelect, AISent, AIBen, AIAcc, AIComplex, AIToolCurrently Using, AIToolInterested in Using, AIToolNot interested in Using, AINextMuch more integrated, AINextNo change, AINextMore integrated, AINextLess integrated, AINextMuch less integrated, AIThreat, AIEthics, AIChallenges, TBranch, ICorPM, WorkExp, Knowledge_1, Knowledge_2, Knowledge_3, Knowledge_4, Knowledge_5, Knowledge_6, Knowledge_7, Knowledge_8, Knowledge_9, Frequency_1, Frequency_2, Frequency_3, TimeSearching, TimeAnswering, Frustration, ProfessionalTech, ProfessionalCloud, ProfessionalQuestion, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 114 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Write your code here\n",
    "# 1. Count the number of duplicate rows\n",
    "df.duplicated().sum()\n",
    "\n",
    "# 2. Display the first few duplicate rows\n",
    "df[df.duplicated()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 2: Remove the duplicate rows from the dataframe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows after removal: 0\n"
     ]
    }
   ],
   "source": [
    "## Write your code here\n",
    "# Remove duplicates (even though Task 1 showed there are none)\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Verify removal by counting duplicates again\n",
    "duplicates_after = df.duplicated().sum()\n",
    "\n",
    "print(\"Number of duplicate rows after removal:\", duplicates_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Forcing pandas to left-align the index\n",
    "# pd.set_option('display.unicode.east_asian_width', True)\n",
    "# pd.set_option('display.unicode.ambiguous_as_wide', True)\n",
    "# pd.set_option('display.colheader_justify', 'left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Finding Missing Values\n",
    "##### Task 3: Find the missing values for all columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      " ResponseId                 0\n",
      "MainBranch                 0\n",
      "Age                        0\n",
      "Employment                 0\n",
      "RemoteWork             10631\n",
      "                       ...  \n",
      "JobSatPoints_11        35992\n",
      "SurveyLength            9255\n",
      "SurveyEase              9199\n",
      "ConvertedCompYearly    42002\n",
      "JobSat                 36311\n",
      "Length: 114, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Write your code here\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 4: Find out how many rows are missing in the column RemoteWork.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing rows in 'RemoteWork': 10631\n"
     ]
    }
   ],
   "source": [
    "## Write your code here\n",
    "missing_employment = df['RemoteWork'].isnull().sum()\n",
    "print(\"Number of missing rows in 'RemoteWork':\", missing_employment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Imputing Missing Values\n",
    "##### Task 5: Find the value counts for the column RemoteWork.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value counts for 'RemoteWork':\n",
      "\n",
      "RemoteWork\n",
      "Hybrid (some remote, some in-person)    23015\n",
      "Remote                                  20831\n",
      "In-person                               10960\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Show the distribution of non-missing values\n",
    "print(\"\\nValue counts for 'RemoteWork':\\n\") \n",
    "print(df['RemoteWork'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 6: Identify the most frequent (majority) value in the RemoteWork column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most frequent value selected for imputation: Hybrid (some remote, some in-person)\n"
     ]
    }
   ],
   "source": [
    "# Find the most frequent value (Mode)\n",
    "# .mode() returns a Series, so we take the first element [0]\n",
    "most_frequent_value = df['RemoteWork'].mode()[0]\n",
    "print(\"\\nMost frequent value selected for imputation:\", most_frequent_value)\n",
    "# print(f\"The most frequent value is: {most_frequent_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 7: Impute (replace) all the empty rows in the column RemoteWork with the majority value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in 'RemoteWork' after imputation: 0\n",
      "Remaining missing values: 0\n"
     ]
    }
   ],
   "source": [
    "## Write your code here\n",
    "# Impute missing values with the most frequent value (no inplace)\n",
    "df['RemoteWork'] = df['RemoteWork'].fillna(most_frequent_value)\n",
    "\n",
    "# Verify that missing values have been handled\n",
    "print(\"\\nMissing values in 'RemoteWork' after imputation:\", df['RemoteWork'].isnull().sum())\n",
    "# print(f\"Remaining missing values: {df['RemoteWork'].isnull().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 8: Check for any compensation-related columns and describe their distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compensation-related columns found:\n",
      " ['CompTotal', 'AIComplex', 'ConvertedCompYearly']\n",
      "\n",
      "Distribution summary for CompTotal:\n",
      "\n",
      "count     3.374000e+04\n",
      "mean     2.963841e+145\n",
      "std      5.444117e+147\n",
      "min       0.000000e+00\n",
      "25%       6.000000e+04\n",
      "50%       1.100000e+05\n",
      "75%       2.500000e+05\n",
      "max      1.000000e+150\n",
      "Name: CompTotal, dtype: float64\n",
      "\n",
      "Distribution summary for AIComplex:\n",
      "\n",
      "count                                             37021\n",
      "unique                                                5\n",
      "top       Good, but not great at handling complex tasks\n",
      "freq                                              12102\n",
      "Name: AIComplex, dtype: object\n",
      "\n",
      "Distribution summary for ConvertedCompYearly:\n",
      "\n",
      "count    2.343500e+04\n",
      "mean     8.615529e+04\n",
      "std      1.867570e+05\n",
      "min      1.000000e+00\n",
      "25%      3.271200e+04\n",
      "50%      6.500000e+04\n",
      "75%      1.079715e+05\n",
      "max      1.625660e+07\n",
      "Name: ConvertedCompYearly, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Identify compensation-related columns by keyword\n",
    "comp_cols = [col for col in df.columns if 'Comp' in col or 'Pay' in col or 'Salary' in col]\n",
    "\n",
    "print(\"Compensation-related columns found:\\n\", comp_cols)\n",
    "\n",
    "# Describe each compensation column\n",
    "for col in comp_cols:\n",
    "    print(f\"\\nDistribution summary for {col}:\\n\")\n",
    "    print(df[col].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compensation patterns you just uncovered\n",
    "\n",
    "- **CompTotal** is clearly corrupted or contains extreme outliers.  \n",
    "  The mean (`2.96e+145`) and max (`1e+150`) are astronomically large—far beyond any real salary. This usually happens when:\n",
    "  - the column contains mixed types (strings accidentally converted to floats)\n",
    "  - respondents entered unrealistic values\n",
    "  - the dataset includes placeholder or synthetic values\n",
    "\n",
    "- **AIComplex** is not a numeric compensation field.  \n",
    "  It’s a categorical column describing how respondents perceive AI’s ability to handle complex tasks.\n",
    "\n",
    "- **ConvertedCompYearly** is the cleanest and most realistic compensation field.  \n",
    "  Its distribution looks typical for global salary data:\n",
    "  - median around **65,000**\n",
    "  - long right tail up to **16 million** (likely outliers but still within numeric range)\n",
    "  - standard deviation large due to global variation\n",
    "\n",
    "### A concise interpretation you can include in your lab\n",
    "\n",
    "- **CompTotal** contains extreme outliers or corrupted values, making it unreliable for analysis without cleaning or winsorization.\n",
    "- **AIComplex** is not a compensation variable and should be excluded from salary analysis.\n",
    "- **ConvertedCompYearly** is the most usable compensation field, showing a realistic salary distribution with a long right tail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# plt.figure(figsize=(14, 6))\n",
    "\n",
    "# # Histogram\n",
    "# plt.subplot(1, 2, 1)\n",
    "# sns.histplot(df['ConvertedCompYearly'], bins=50, kde=True, color='teal')\n",
    "# plt.title(\"ConvertedCompYearly Distribution\")\n",
    "# plt.xlabel(\"Yearly Compensation\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "\n",
    "# # Boxplot\n",
    "# plt.subplot(1, 2, 2)\n",
    "# sns.boxplot(x=df['ConvertedCompYearly'], color='teal')\n",
    "# plt.title(\"ConvertedCompYearly Boxplot (Outliers Visible)\")\n",
    "# plt.xlabel(\"Yearly Compensation\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Histogram\n",
    "# df['ConvertedCompYearly'].plot(kind='hist', bins=50, figsize=(12, 5), color='teal', title='ConvertedCompYearly Distribution')\n",
    "# plt.xlabel(\"Yearly Compensation\")\n",
    "# plt.show()\n",
    "\n",
    "# # Boxplot\n",
    "# df['ConvertedCompYearly'].plot(kind='box', figsize=(10, 4), color='teal', title='ConvertedCompYearly Boxplot')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. List all columns related to compensation to identify them\n",
    "# comp_cols = [col for col in df.columns if 'Comp' in col]\n",
    "# print(\"Compensation-related columns found:\", comp_cols)\n",
    "\n",
    "# # 2. Describe the distribution (Statistical Summary)\n",
    "# # This shows count, mean, std, min, 25%, 50% (median), 75%, and max\n",
    "# print(\"\\nStatistical Summary of Compensation:\")\n",
    "# print(df['ConvertedCompYearly'].describe())\n",
    "\n",
    "# # 3. Visualize the Distribution\n",
    "# plt.figure(figsize=(12, 6))\n",
    "\n",
    "# # Use a Histogram with a Kernel Density Estimate (KDE) line\n",
    "# sns.histplot(df['ConvertedCompYearly'], bins=50, kde=True, color='forestgreen')\n",
    "\n",
    "# plt.title('Distribution of Annual Compensation (ConvertedCompYearly)', fontsize=15)\n",
    "# plt.xlabel('Annual Compensation (USD)', fontsize=12)\n",
    "# plt.ylabel('Frequency', fontsize=12)\n",
    "\n",
    "# # Optional: Adding a vertical line for the Median\n",
    "# median_val = df['ConvertedCompYearly'].median()\n",
    "# plt.axvline(median_val, color='red', linestyle='--', label=f'Median: {median_val:,.0f}')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this lab, you focused on imputing missing values in the dataset.**\n",
    "\n",
    "- Use the <code>pandas.read_csv()</code> function to load a dataset from a CSV file into a DataFrame.\n",
    "\n",
    "- Download the dataset if it's not available online and specify the correct file path.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2024-11-05|1.3|Madhusudhan Moole|Updated lab|\n",
    "|2024-10-29|1.2|Madhusudhan Moole|Updated lab|\n",
    "|2024-09-27|1.1|Madhusudhan Moole|Updated lab|\n",
    "|2024-09-26|1.0|Raghul Ramesh|Created lab|\n",
    "--!>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "70ab641719bca2be0bdcb38f6a8b5de7851b6e9c28d41b9407096c62e74916a6"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
